{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9076d4cb-c9e3-49a5-b42e-ece1692dc276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2298af4-ea0e-4ad7-b73b-82f0b412b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DoubleConv class for UNet\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        #  Apply 2D convolution to extract spatial features, normalisation to speed up training and reduce over fitting, ReLU to learn complex functions\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    # Define data flow\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a96bdc-629e-4c47-892a-57c97b9745cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DownSample class for UNet\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Define data flow\n",
    "    def forward(self, x):\n",
    "        skip = self.conv(x)\n",
    "        x = self.pool(skip)\n",
    "        return x, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a54b755-732c-44ff-94a7-b659b4b53da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UpSample class for UNet\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_channels * 2, out_channels)  # Concatenate skip connection\n",
    "\n",
    "    # Define data flow\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upconv(x)\n",
    "        # Align the skip connection's shape with the upsampled x\n",
    "        x = torch.cat([x, skip], dim=1)  # Concatenate along the channel dimension\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f93d9f-382c-485e-8bff-6cf65570d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNet class\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Downsampling\n",
    "        self.downs = nn.ModuleList([\n",
    "            DownSample(in_channels, 64),\n",
    "            DownSample(64, 128),\n",
    "            DownSample(128, 256),\n",
    "            DownSample(256, 512)\n",
    "        ])\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Upsampling\n",
    "        self.ups = nn.ModuleList([\n",
    "            UpSample(1024, 512),\n",
    "            UpSample(512, 256),\n",
    "            UpSample(256, 128),\n",
    "            UpSample(128, 64)\n",
    "        ])\n",
    "\n",
    "        # Final output layer\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    # Define data flow\n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        skip_connections = []\n",
    "        for down_conv in self.downs:\n",
    "            x, skip = down_conv(x)\n",
    "            skip_connections.append(skip)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Upsampling path with skip connections\n",
    "        for up_conv, skip in zip(self.ups, reversed(skip_connections)):\n",
    "            x = up_conv(x, skip)\n",
    "        \n",
    "        # Final output layer\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06660e3d-a067-4882-95ee-cd27df9296f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.pt')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.pt')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(os.path.join(self.image_dir, self.image_files[idx]))\n",
    "        mask = torch.load(os.path.join(self.mask_dir, self.mask_files[idx]))\n",
    "\n",
    "        # Fix image shape (C, H, W)\n",
    "        image = image.squeeze(0).permute(2, 0, 1)\n",
    "        image = image.float()\n",
    "\n",
    "        # Fix mask shape (C, H, W)\n",
    "        mask = (mask > 0).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18436e5-16e2-46f1-88f7-54a7a28f29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train paths\n",
    "train_image_dir = r'E:\\processed_tiles\\train\\images'\n",
    "train_mask_dir = r'E:\\processed_tiles\\train\\masks'\n",
    "\n",
    "train_dataset = SegmentationDataset(train_image_dir, train_mask_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Test paths\n",
    "test_image_dir = r'E:\\processed_tiles\\test\\images'\n",
    "test_mask_dir = r'E:\\processed_tiles\\test\\masks'\n",
    "\n",
    "test_dataset = SegmentationDataset(test_image_dir, test_mask_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cab9a4f-91be-4939-aa8c-a3f7cd138c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([8, 3, 128, 128])\n",
      "Mask batch shape: torch.Size([8, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Confirm shape\n",
    "for images, masks in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Mask batch shape:\", masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb8993-cc15-459b-8aeb-8991e7271f28",
   "metadata": {},
   "source": [
    "This is how we want the data to be formated for the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca5f32d-d42a-4535-9d31-e10829fbec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image min/max: 0.003921568859368563 0.5411764979362488\n",
      "Mask unique values: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Confirm between 0 and 1\n",
    "for images, masks in train_loader:\n",
    "    print(\"Image min/max:\", images.min().item(), images.max().item())\n",
    "    print(\"Mask unique values:\", torch.unique(masks))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453b5a7-12c7-4a95-9c01-e3541be07580",
   "metadata": {},
   "source": [
    "Max is a bit lower than 1 but, it should still work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d191f1-7d9e-4ce3-8997-bdea01f8406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3031165373445211\n",
      "Epoch [2/10], Loss: 0.14142448758839254\n",
      "Epoch [3/10], Loss: 0.08348095979561972\n",
      "Epoch [4/10], Loss: 0.05804115713623368\n",
      "Epoch [5/10], Loss: 0.045095034434002104\n",
      "Epoch [6/10], Loss: 0.03731173060330438\n",
      "Epoch [7/10], Loss: 0.03338015533581894\n",
      "Epoch [8/10], Loss: 0.028566366440396913\n",
      "Epoch [9/10], Loss: 0.025337434817323465\n",
      "Epoch [10/10], Loss: 0.02500653194290819\n"
     ]
    }
   ],
   "source": [
    "# Initialise UNet model\n",
    "model = UNet(in_channels=3, num_classes=1).cuda()  # Use GPU\n",
    "\n",
    "# Define loss function and optimiser\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # For binary segmentation\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.cuda(), masks.cuda()  # Move data to GPU if available\n",
    "        \n",
    "        optimiser.zero_grad()  # Zero out the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
